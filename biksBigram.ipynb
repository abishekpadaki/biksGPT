{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "this is the first primer of sorts to understanding a transformer. we build a bigram model which takes in a shakespeare dataset, and trains to predict the next token, but considering just 1 past token.\n",
        "\n",
        "this would give us a good intuition for the future transformer model we will build.\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "PNQbKnKihEQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will begin by download the tiny shakespeare dataset, which consists of all of his works in a single txt file\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inaqk3iohDoW",
        "outputId": "4ebf64c1-dee0-4354-c06d-3f44492ec155"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-02 04:17:51--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-03-02 04:17:51 (25.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZlmLhdD8f5sP"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of the dataset (num of characters):\", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZndOTXGiGgQ",
        "outputId": "3718400a-50fc-4aca-d5f5-8fbaaae088bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the dataset (num of characters): 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring the vocabulary of our dataset to see the different characters present in it\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f'Size if vocab is: {vocab_size}\\n and the characters are: {\"\".join(chars)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6XZ4EV2iPzx",
        "outputId": "21755e9b-d14d-4a50-db1e-9ac7ea13d393"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size if vocab is: 65\n",
            " and the characters are: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building our simple Tokenizer that simply converts our vocab to a unique integer\n",
        "ch_to_int = {ch:i for i,ch in enumerate(chars)}\n",
        "int_to_ch = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "encoder = lambda str: [ch_to_int[char] for char in str] #encodes each character in a string with it's unique int val\n",
        "decoder = lambda int: ''.join(int_to_ch[i] for i in int) #decodes the characters in their integer forms"
      ],
      "metadata": {
        "id": "izmrJEKBjBMA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing out our tokenizer\n",
        "print(encoder('penguins are the best'))\n",
        "print(decoder(encoder('penguins are the best')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcIk-QtFnLs1",
        "outputId": "6b5102ae-9ff4-4de2-e853-a3b696c5ba2b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[54, 43, 52, 45, 59, 47, 52, 57, 1, 39, 56, 43, 1, 58, 46, 43, 1, 40, 43, 57, 58]\n",
            "penguins are the best\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#applying our tokenizer to our dataset using pytorch\n",
        "\n",
        "import torch\n",
        "data = torch.tensor(encoder(text), dtype=torch.long) #encode and convert to torch type tensor\n",
        "print(data.shape, data.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptmr0NH0naka",
        "outputId": "307f72fb-4144-427a-a728-b4d78a2c5f54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's look at a small part of our dataset, before and after encoding\n",
        "print('Before:')\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK3LCFKtnvyI",
        "outputId": "d66c4fbf-19e8-4dd3-f8e1-08adec77676f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('After:')\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwcXHpTmoPZ_",
        "outputId": "418ca717-1747-4984-f97f-42882596b1e5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After:\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting our dataset ready for training\n",
        "\n",
        "#first we split our dataset into train and test sets.\n",
        "# 80% will be kept for train and the rest for test\n",
        "\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]\n",
        "\n"
      ],
      "metadata": {
        "id": "hWHAYcqJoUHw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chunking or splitting up the train set into chunks/blocks of size 9\n",
        "\n",
        "# we chunk them up so that the model can learn the importance of 9 characters and their relationship with each other, such that given the first char it will be able to predict the next token (char).\n",
        "#this is the max context size/length for predictions\n",
        "chunk_size = 9\n",
        "\n",
        "#example\n",
        "print(train_data[:chunk_size+1])\n",
        "print(decoder((train_data[:chunk_size+1]).numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YxKC1zXpB_D",
        "outputId": "fc6d6a85-a9f1-4665-eb7b-e6a224d8e4f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
            "First Citi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's demonstrate how the above would work\n",
        "\n",
        "x = train_data[:chunk_size]\n",
        "y = train_data[1:chunk_size+1]\n",
        "\n",
        "for t in range(chunk_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ7Kx0ZouWfB",
        "outputId": "da9b985d-af18-4f4d-cc52-dd2f61b796a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) the target: 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's split our training data into batches for efficient parallelism\n",
        "\n",
        "torch.manual_seed(22)\n",
        "batch_size = 4 #split our training to 4 parallel batches\n",
        "chunk_size = 9\n",
        "\n",
        "def fetch_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - chunk_size, (batch_size,)) #returns batch_size number of random integers between 0 and len(data) - chunksize\n",
        "  #These random integers will be used as starting indices for selecting chunks of data.\n",
        "  #now that we have a batch of random parts of our data, we classify them into x (dependent vars) and y (target)\n",
        "  x = torch.stack([data[i:i+chunk_size] for i in ix])\n",
        "  #here x is a stack or multiple rows (batch size num of rows) to act as input data\n",
        "  y = torch.stack([data[i+1:i+chunk_size+1] for i in ix])\n",
        "  #here y is a stack or multiple rows to act as target/output data for each tensor rows in input\n",
        "\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "ooYnxrkorVh0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = fetch_batch('train')\n",
        "print('The inputs are:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "\n",
        "print('The ouputs are:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "#As you can see below the input and outputs will be of shape [batch_size, chunk_size]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-jV3Q0tyirK",
        "outputId": "dcdbccdd-9c94-4d04-b11c-89928dc4b6f7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The inputs are:\n",
            "torch.Size([4, 9])\n",
            "tensor([[44,  8,  0, 32, 46, 47, 57,  1, 56],\n",
            "        [50, 47, 60, 43,  0, 32, 53,  1, 40],\n",
            "        [61, 52,  6,  0, 31, 47, 52, 41, 43],\n",
            "        [ 1, 40, 43,  1, 53, 52, 43,  8,  0]])\n",
            "The ouputs are:\n",
            "torch.Size([4, 9])\n",
            "tensor([[ 8,  0, 32, 46, 47, 57,  1, 56, 53],\n",
            "        [47, 60, 43,  0, 32, 53,  1, 40, 43],\n",
            "        [52,  6,  0, 31, 47, 52, 41, 43,  1],\n",
            "        [40, 43,  1, 53, 52, 43,  8,  0, 14]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's demonstrate a similar prediction example like before:\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(chunk_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOEb49Fiywe-",
        "outputId": "b9d6ec27-31e6-42c7-d63c-cd9f43122bd9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is [44] the target: 8\n",
            "when input is [44, 8] the target: 0\n",
            "when input is [44, 8, 0] the target: 32\n",
            "when input is [44, 8, 0, 32] the target: 46\n",
            "when input is [44, 8, 0, 32, 46] the target: 47\n",
            "when input is [44, 8, 0, 32, 46, 47] the target: 57\n",
            "when input is [44, 8, 0, 32, 46, 47, 57] the target: 1\n",
            "when input is [44, 8, 0, 32, 46, 47, 57, 1] the target: 56\n",
            "when input is [44, 8, 0, 32, 46, 47, 57, 1, 56] the target: 53\n",
            "when input is [50] the target: 47\n",
            "when input is [50, 47] the target: 60\n",
            "when input is [50, 47, 60] the target: 43\n",
            "when input is [50, 47, 60, 43] the target: 0\n",
            "when input is [50, 47, 60, 43, 0] the target: 32\n",
            "when input is [50, 47, 60, 43, 0, 32] the target: 53\n",
            "when input is [50, 47, 60, 43, 0, 32, 53] the target: 1\n",
            "when input is [50, 47, 60, 43, 0, 32, 53, 1] the target: 40\n",
            "when input is [50, 47, 60, 43, 0, 32, 53, 1, 40] the target: 43\n",
            "when input is [61] the target: 52\n",
            "when input is [61, 52] the target: 6\n",
            "when input is [61, 52, 6] the target: 0\n",
            "when input is [61, 52, 6, 0] the target: 31\n",
            "when input is [61, 52, 6, 0, 31] the target: 47\n",
            "when input is [61, 52, 6, 0, 31, 47] the target: 52\n",
            "when input is [61, 52, 6, 0, 31, 47, 52] the target: 41\n",
            "when input is [61, 52, 6, 0, 31, 47, 52, 41] the target: 43\n",
            "when input is [61, 52, 6, 0, 31, 47, 52, 41, 43] the target: 1\n",
            "when input is [1] the target: 40\n",
            "when input is [1, 40] the target: 43\n",
            "when input is [1, 40, 43] the target: 1\n",
            "when input is [1, 40, 43, 1] the target: 53\n",
            "when input is [1, 40, 43, 1, 53] the target: 52\n",
            "when input is [1, 40, 43, 1, 53, 52] the target: 43\n",
            "when input is [1, 40, 43, 1, 53, 52, 43] the target: 8\n",
            "when input is [1, 40, 43, 1, 53, 52, 43, 8] the target: 0\n",
            "when input is [1, 40, 43, 1, 53, 52, 43, 8, 0] the target: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we mainly capture x(input) for the model to train on and the target y (output) would be used to calculate the loss"
      ],
      "metadata": {
        "id": "fEzNLYKnzTcN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's get to the juict stuff and build our NN architecture"
      ],
      "metadata": {
        "id": "WkAmVEomzz8c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we start with a bigram model\n",
        "#bigram is when the model take 2(bi) words and trains on them to figure the realtion between them.\n",
        "#ex 'Machine Learning' - the importance of 'Learning' is computed by looking at one word back i.e 'Machine'"
      ],
      "metadata": {
        "id": "6jfqa9mh4grq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(22)\n",
        "\n",
        "class BigramModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "    #this is a lookup or hashmap table of vocab_size x vocab_size, where each token directly reads off the logits (scores) for next token from this table\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "\n",
        "    #idx and targets here are the (B,T) tensors of integers we split before.\n",
        "\n",
        "    logits = self.token_embedding_table(idx) # (B,T,C) pytorch converts to torch of shape (batch, target, channels) here channels is vocab_size\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      #we need to reshape our logits as pytorch expects shape (B*T, C) for input for cross entropy, so let's modify that\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  #now let's define our generate function which will predict the next token\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    #idx is (B,T) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self(idx) #get the predictions\n",
        "      #we want to focus only on last timestamp to predict next token\n",
        "      logits = logits[:, -1, :] #becomes (B,C)\n",
        "      #apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B,C)\n",
        "      #sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
        "      #append sampled index to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "kVzIHGNL5XwN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's test this out\n",
        "\n",
        "m = BigramModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape) #shape of logits (B,T,C)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv2H9DlZ7XFq",
        "outputId": "3c2924c7-0d68-4343-93ff-0d353f329c80"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([36, 65])\n",
            "tensor(4.8492, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)\n",
        "generated_token = m.generate(idx, max_new_tokens=100)\n",
        "print(decoder(generated_token[0].tolist()))\n",
        "\n",
        "#before training, we get some garbage as you can see"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjrCZifA7cg3",
        "outputId": "a25a4695-6e49-4f88-8d2f-a66964427c11"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "flA-uPX\n",
            "tZOF3LP&XTdoIVYgz,chayxaUmDOEnafCAZrnWIRDcbK3K.DFNQYHJ&rnHOG&It!?DcGTmCGBW,bfFeFYyis3Wk-jCuK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so now it is training time. yum.\n",
        "\n",
        "#create a pytorch optimizer. we will use AdamW. You can also use SGD\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "AIltyE1gBmoS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train loop\n",
        "\n",
        "batch_size = 32\n",
        "for steps in range (10000):\n",
        "  #let us sample a batch of the data\n",
        "  xb,yb = fetch_batch('train')\n",
        "\n",
        "  #we now evaluate the loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR0p7KK_C9Ek",
        "outputId": "cc937e36-5807-42f3-87c6-846c655d9817"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4050614833831787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decoder(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n",
        "\n",
        "#not great but we getting some sentence structure! so decent progress"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1lJUPUXDoy8",
        "outputId": "e612d7d7-d800-49a0-edf3-09bfecc7e1f1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Foul, wse mbikefota y n!\n",
            "I anclivelle ar:\n",
            "HNCAwipre soseatilibl:\n",
            "Whe birst penser un wo ad he d, ces imivethourthay, s c me tooraknatt.\n",
            "FFisthoryon I ald vellllt t mu t wetee, ithot ucowhrwnkld cuchy fr thep.\n",
            "\n",
            "Fors be, waler g ghe ol anl mowe feve, ay ven s\n",
            "SBy th oelean flk, weolD:\n",
            "\n",
            "Thoome t buspomy thens tathe tol yay t byooofupt\n",
            "Hin wacunske mothowhiserup, f he bl que uPr ccheawead thar athimigo pon arfonnth blond l ild ERERMPam a at p! aly RD:\n",
            "ISOnce\n",
            "Thean yourerresq:\n",
            "\n",
            "\n",
            "\n",
            "Whantoouneep mirdlon\n"
          ]
        }
      ]
    }
  ]
}